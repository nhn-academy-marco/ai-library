# 01. 성능과 비용: AI 서비스의 현실적인 문제와 해결책

## 학습 전제조건

이 문서를 학습하기 전에 다음 내용을 알고 있으면 도움이 됩니다:
- Week 1~3의 RAG 검색 시스템 이해
- API 호출과 응답 시간 개념
- 토큰(Token) 기초 지식

---

## 1. 개요

AI 서비스를 운영하면 **세 가지 큰 문제**에 직면합니다:

1. **응답이 너무 느리다** → 사용자가 답답해함
2. **비용이 너무 많이 든다** → 운영 비용 폭발
3. **품질과 비용의 트레이드오프** → 절감하면 품질이 떨어질까?

**이 문서에서 배울 내용:**
- RAG 시스템에서 발생하는 성능 문제의 원인
- 토큰이 비용과 속도에 미치는 영향
- **실제 프로젝트에 구현된 최적화 기술** 2가지
  - 시맨틱 캐싱 (03.semantic-caching.md)
  - Top-K 최적화 (02.topk-optimization.md)

> **실제 프로젝트 구현 상태**
>
> 이 문서는 **구현된 최적화 기술**을 중심으로 설명합니다.
>
> ✅ **실제 구현된 기능 (상세 구현은 각 문서 참고):**
> - 시맨틱 캐싱 (03.semantic-caching.md)
> - Top-K 최적화 (02.topk-optimization.md)
>
> ⚠️ **개념만 이해하는 부분:**
> - 토큰/비용 계산 (이해 목적)
> - 성능 모니터링 (로그 확인 수준)

---

## 2. AI 서비스의 3대 성능 문제

### 2.1. 문제 1: 응답 지연 (Latency)

**현상:**
사용자가 질문하고 AI가 답변할 때까지 너무 오래 기다려야 합니다.

**지연 시간 분석:**
```
[일반적인 검색엔진]
사용자 질문 → 검색 → 0.1초 후 결과
사용자: "빠르다!"

[RAG 검색 (최적화 전)]
사용자 질문 → 검색(0.5초) → 임베딩(0.3초) → AI 호출(2~3초)
총 2.8~3.8초
사용자: "느리다..."
```

**지연 구성 요소:**
```
총 지연 = DB 쿼리 + 임베딩 + 벡터 검색 + AI 생성

가장 큰 비중:
1. AI 생성: 1~3초 (40~60%)
2. 벡터 검색: 200~500ms (20~30%)
3. DB 쿼리: 50~200ms (10~20%)
4. 임베딩: 100~300ms (10~20%)
```

**사용자 경험:**
- **1초 이하**: 빠름, 만족
- **1~2초**: 적당함
- **2~3초**: 조금 답답함
- **3초 이상**: 매우 답답함, 이탈 가능성 높음

---

### 2.2. 문제 2: 토큰 폭발 (Token Explosion)

**현상:**
검색 결과를 AI에게 너무 많이 전달하면 토큰이 폭발적으로 증가합니다.

**토큰이란?**
AI가 텍스트를 처리하는 기본 단위
- 한글: 1글자 ≈ 0.5토큰
- 영어: 1단어 ≈ 0.7토큰

**문제 상황:**
```
[Top-K = 100]
검색 결과: 100권
도서당 평균 내용: 200자
총 컨텍스트: 20,000자

토큰 수: 20,000자 ÷ 2 = 10,000토큰
비용 (Gemini 2.0 Flash):
  입력: 10,000 × $0.10 / 1M = $1.00/요청
  1일 1,000요청: $1,000/일 → 월 $30,000
```

**비용 체감:**
```
사용자 1,000명이 하루에 2번 검색:
- K=100: 월 $60,000
- K=5:   월 $3,000  (95% 절감)
- K=3:   월 $1,800  (97% 절감)
```

---

### 2.3. 문제 3: 품질 vs 비용 딜레마

**Trade-off:**
```
[너무 적음: K=3]
✓ 비용: 저렴
✓ 속도: 빠름
✗ 품질: 정보 부족, 정답 누락

[너무 많음: K=100]
✗ 비용: 매우 높음
✗ 속도: 느림
✓ 품질: 정보는 충분하지만 노이즈 많음

[적절함: K=5]
✓ 비용: 중간
✓ 속도: 빠름
✓ 품질: 높음
→ 최적의 균형
```

**핵심 질문:**
> "어떻게 비용과 속도를 희생하지 않고 품질을 유지할 수 있을까?"

---

## 3. 실제 구현된 해결책 1: 시맨틱 캐싱

### 3.1. 문제: 반복 질문의 낭비

**현상:**
```
사용자 A: "자바 책 추천해줘" → AI 호출 ($0.10)
사용자 B: "자바 책 추천해줘" → AI 호출 ($0.10)
사용자 C: "자바 책 추천해줘" → AI 호출 ($0.10)
...

동일한 질문 100번 = $10 낭비
```

### 3.2. 해결: 시맨틱 캐싱

**개념:**
동일한 질문이나 유사한 질문이 오면, AI를 다시 호출하지 않고 이전 결과를 재사용합니다.

**실제 구현 (03.semantic-caching.md):**

| 구현 요소 | 설정값 | 설명 |
|----------|--------|------|
| 유사도 기준 | Cosine Similarity ≥ 0.98 | 98% 이상 유사하면 동일 질문으로 간주 |
| 캐시 저장소 | Redis | 빠른 인메모리 DB |
| TTL | 30분 | 30분 후 캐시 만료 |
| 웜업 | 인기 검색어 사전 캐싱 | 서비스 시작 시 미리 캐시 생성 |

**Redis 설정 (redis.properties):**
```properties
# Redis Settings
# Redis 설정이 있으면 RedisConfig가 자동으로 활성화됩니다
spring.data.redis.host=s4.java21.net
spring.data.redis.port=6379
spring.data.redis.password=<비밀번호>
spring.data.redis.database=0
```

**Redis 접속 확인:**
```bash
# Redis 서버 연결 테스트
$ redis-cli -h s4.java21.net -p 6379 -a <비밀번호>

# 캐시 확인
redis> KEYS semantic:cache:*
1) "semantic:cache:abcd1234..."

# 캐시 내용 확인
redis> GET semantic:cache:abcd1234...
"{\"query\":\"자바 책 추천\",\"result\":...}"

# TTL 확인
redis> TTL semantic:cache:abcd1234...
(integer) 1745  # 남은 시간 (초 단위, 30분 = 1800초)
```

**효과:**
```
[캐싱 전]
동일 질문 100회 = AI 호출 100회 = $10

[캐싱 후]
1회째: AI 호출 ($0.10) → 캐시 저장
2~100회: 캐시 사용 ($0.00)
총비용: $0.10 (99% 절감)

응답 시간:
  AI 호출: 2~3초
  캐시 Hit: 0.1초 이내 (20~30배 빨라짐)
```

**실제 코드 (개요):**
```java
// SemanticCacheService.java
@Service
@Slf4j
public class SemanticCacheService {

    private static final double SIMILARITY_THRESHOLD = 0.98;
    private static final int CACHE_TTL_MINUTES = 30;

    public CachedResult findCachedResult(String query) {
        log.info("[캐시 조회] 쿼리: {}", query);
        log.debug("[캐시 조회] 유사도 임계값: {}", SIMILARITY_THRESHOLD);

        // 1. 쿼리 임베딩
        float[] queryEmbedding = embeddingService.embed(query);
        log.debug("[캐시 조회] 임베딩 완료: {} 차원", queryEmbedding.length);

        // 2. 캐시된 결과들과 유사도 계산
        List<CachedQuery> cachedQueries = cacheRepository.findAll();
        log.debug("[캐시 조회] 캐시된 항목 수: {}개", cachedQueries.size());

        // 3. 코사인 유사도 0.98 이상이면 재사용
        for (CachedQuery cached : cachedQueries) {
            double similarity = cosineSimilarity(queryEmbedding, cached.getEmbedding());
            log.trace("[캐시 비교] 유사도: {:.4f} (기준: {:.4f})",
                similarity, SIMILARITY_THRESHOLD);

            if (similarity >= SIMILARITY_THRESHOLD) {
                log.info("[캐시 Hit] 유사도: {:.4f}, 캐시된 쿼리: {}",
                    similarity, cached.getQuery());
                return cached.getResult();
            }
        }

        log.info("[캐시 Miss] 유사한 캐시 없음, AI 호출 필요");
        return null;  // 캐시 Miss
    }

    public void saveToCache(String query, CachedResult result) {
        log.info("[캐시 저장] 쿼리: {}, TTL: {}분",
            query, CACHE_TTL_MINUTES);
        // 캐시 저장 로직...
        log.debug("[캐시 저장] 완료, 만료 시각: {}분 후",
            CACHE_TTL_MINUTES);
    }
}
```

**학습 포인트:**
- 코사인 유사도(Cosine Similarity)로 유사 질문 판별
- Redis로 빠른 캐시 구현
- TTL로 캐시 만료 정책 관리
- 웜업으로 초기 성능 확보

---

## 4. 실제 구현된 해결책 2: Top-K 최적화

### 4.1. 문제: 너무 많은 검색 결과

**현상:**
```
[문제: K=100]
1. DB에서 100권 검색 (Retrieval)
2. 100권을 AI에게 전달 (Rerank 없음)
3. AI가 100권 분석

문제점:
- 토큰 폭발: 10,000토큰
- 비용 폭발: $1.00/요청
- 노이즈 증가: 관련 없는 도서도 포함
- 응답 지연: AI 처리 시간 5초 이상
```

### 4.2. 해결: Retrieval K vs Rerank K 분리

**개념:**
```
[Retrieval K]: 검색 엔진에서 가져오는 도서 수
  → 충분한 후보군 확보: 50~100개

[Rerank K]: AI에게 전달하는 도서 수
  → 관련 있는 것만 정선: 5~10개
```

**실제 구현 (02.topk-optimization.md):**

| 구현 요소 | 설정값 | 설명 |
|----------|--------|------|
| Retrieval K | 100개 | 검색 엔진에서 100권 가져옴 |
| Rerank K | 5개 | AI에게 상위 5권만 전달 |
| RRF 점수 임계값 | 0.02 | 점수 0.02 미만은 필터링 |
| 정렬 기준 | RRF 점수 내림차순 | 관련성 높은 순서대로 |

**효과:**
```
[최적화 전: K=100]
컨텍스트: 100권 × 200자 = 20,000자
토큰: 약 10,000토큰
비용: $1.00/요청
응답 시간: 5초

[최적화 후: Retrieval 100 → Rerank 5]
컨텍스트: 5권 × 200자 = 1,000자
토큰: 약 500토큰
비용: $0.05/요청
응답 시간: 1.5초

절감 효과:
  토큰: 95% 절감
  비용: 95% 절감
  시간: 70% 단축
```

**실제 코드 (개요):**
```java
// RagSearchStrategy.java
@Service
@Slf4j
public class RagSearchStrategy {

    private static final int DEFAULT_BATCH_SIZE = 100;    // Retrieval K
    private static final int MAX_AI_CANDIDATES = 5;        // Rerank K
    private static final double SCORE_THRESHOLD = 0.02;    // RRF 임계값

    public List<BookSearchResponse> selectTopKBooks(SearchResult retrievalResult) {
        List<BookSearchResponse> allBooks = retrievalResult.getBooks().getContent();
        log.info("[Top-K 최적화] 시작");
        log.info("[Top-K 최적화] Retrieval K: {}권 (검색된 도서)",
            allBooks.size());
        log.debug("[Top-K 최적화] RRF 임계값: {}", SCORE_THRESHOLD);
        log.debug("[Top-K 최적화] Rerank K: {}권 (AI에게 전달할 최대 도서)",
            MAX_AI_CANDIDATES);

        // 필터링 전 통계
        long scoreNullCount = allBooks.stream()
            .filter(b -> b.getRrfScore() == null).count();
        log.debug("[Top-K 최적화] RRF 점수 없는 도서: {}권", scoreNullCount);

        // Stream으로 필터링 및 정렬
        List<BookSearchResponse> topKBooks = allBooks.stream()
            // 1. RRF 점수 있는 것만
            .filter(book -> {
                boolean hasScore = book.getRrfScore() != null;
                if (!hasScore) {
                    log.trace("[필터링] RRF 점수 없음: ID={}, 제목={}",
                        book.getId(), book.getTitle());
                }
                return hasScore;
            })
            // 2. 임계값 이상만 (노이즈 제거)
            .filter(book -> {
                double score = book.getRrfScore();
                boolean pass = score >= SCORE_THRESHOLD;
                if (!pass) {
                    log.trace("[필터링] RRF 점수 미달: ID={}, 제목={}, 점수={:.4f}",
                        book.getId(), book.getTitle(), score);
                }
                return pass;
            })
            // 3. RRF 점수 높은 순 정렬
            .sorted((b1, b2) -> Double.compare(b2.getRrfScore(), b1.getRrfScore()))
            // 4. 상위 5개만 선택 (Rerank K)
            .limit(MAX_AI_CANDIDATES)
            .toList();

        // 결과 로깅
        log.info("[Top-K 최적화] 완료");
        log.info("[Top-K 최적화] 필터링: {}권 → Rerank: {}권 (절감율: {:.1f}%)",
            allBooks.size(),
            topKBooks.size(),
            (1 - (double) topKBooks.size() / allBooks.size()) * 100);

        if (!topKBooks.isEmpty()) {
            double minScore = topKBooks.get(topKBooks.size() - 1).getRrfScore();
            double maxScore = topKBooks.get(0).getRrfScore();
            log.info("[Top-K 최적화] RRF 점수 범위: {:.4f} ~ {:.4f}",
                minScore, maxScore);

            log.debug("[Top-K 최적화] 선정된 도서:");
            for (int i = 0; i < topKBooks.size(); i++) {
                BookSearchResponse book = topKBooks.get(i);
                log.debug("  {}. {} (RRF: {:.4f})",
                    i + 1, book.getTitle(), book.getRrfScore());
            }
        }

        return topKBooks;
    }
}
```

**학습 포인트:**
- Retrieval과 Rerank 분리의 중요성
- RRF(Reciprocal Rank Fusion) 점수 기반 필터링
- Stream API를 활용한 필터링 및 제한
- 노이즈 제거로 품질 유지

---

## 5. 두 기술의 시너지 효과

### 5.1. 개별 효과

| 기술 | 주요 해결 문제 | 효과 |
|------|--------------|------|
| 시맨틱 캐싱 | 반복 질문 | 99% 비용 절감, 20~30배 속도 향상 |
| Top-K 최적화 | 토큰 폭발 | 95% 비용 절감, 70% 속도 향상 |

### 5.2. 결합 효과

```
[최적화 전]
사용자 1,000명, 1인 2회/일, K=100
월 비용: $60,000
평균 응답 시간: 3초

[캐싱만 적용]
월 비용: $30,000 (50% 절감)
평균 응답 시간: 1.5초 (50% 단축)

[Top-K만 적용]
월 비용: $3,000 (95% 절감)
평균 응답 시간: 1초 (67% 단축)

[둘 다 적용]
월 비용: $1,500 (97.5% 절감) ✅
평균 응답 시간:
  - 첫 요청: 1초
  - 캐시 Hit: 0.1초
```

### 5.3. 로그로 확인하는 성능

**실제 운영 로그 예시:**
```log
[SemanticCache] 캐시 Miss: "자바 책 추천해줘" → AI 호출
[RagSearchStrategy] Retrieval: 100권 → Rerank: 5권 (RRF ≥ 0.02)
[AiService] 토큰 사용: 입력 500토큰, 출력 300토큰, 비용 $0.17
[Performance] 응답 시간: 1.2초

[SemanticCache] 캐시 Hit: "자바 책 추천해줘" (유사도: 0.992)
[Performance] 응답 시간: 0.08초 (캐시 재사용)
```

---

## 6. 추가 최적화 전략 (개념 이해)

### 6.1. 컨텍스트 요약

도서 내용이 길면 요약해서 전달합니다.

```
[전]
도서 내용: 500자 전체 전달
토큰: 250토큰

[후]
도서 내용: "자바의 기초부터 객체 지향까지 다루는 입문서입니다."
토큰: 20토큰 (92% 절감)
```

### 6.2. 토큰 제한 설정

AI 모델의 최대 토큰 제한을 설정합니다.

```
Gemini 2.0 Flash:
- 최대 입력: 1,000,000토큰
- 권장 입력: 30,000토큰 이하
- 우리 프로젝트: 1,000토큰 목표
```

---

## 7. 학습 체크리스트

다음 내용을 이해했는지 확인해 보세요:

### 성능 문제 이해
- [ ] RAG 시스템에서 왜 응답이 느린지 안다 (검색 + 임베딩 + AI)
- [ ] 토큰이 왜 비용 문제를 일으키는지 안다
- [ ] 품질과 비용의 트레이드오프를 이해한다

### 해결책 이해
- [ ] 시맨틱 캐싱이 무엇인지 안다
- [ ] Top-K 최적화가 무엇인지 안다
- [ ] Retrieval K와 Rerank K의 차이를 안다

### 실제 구현 연결
- [ ] 시맨틱 캐싱이 03.semantic-caching.md에 구현되어 있음
- [ ] Top-K 최적화가 02.topk-optimization.md에 구현되어 있음
- [ ] 두 기술이 함께 사용될 때 시너지 효과를 안다

---

## 8. 다음 단계

이 문서에서는 **성능 문제와 해결책의 개요**를 학습했습니다.

다음 문서들은 **실제 구현된 최적화 기술**을 상세히 다룹니다:

**02. Top-K와 컨텍스트 최적화** → 다음 학습 추천
- Retrieval K와 Rerank K 상세 구현
- RRF 점수 기반 필터링
- Stream API 활용
- K 값 결정 가이드
- 품질 vs 비용 균형

**03. 시맨틱 캐싱**
- 코사인 유사도 알고리즘
- Redis 캐시 구현
- 웜업 전략
- TTL 정책
- 캐시 적중률 최적화

---

## FAQ

### Q1: 왜 K=5로 선택했나요?

**A:** 실험 결과입니다.
- K=3: 정보가 부족해서 사용자 만족도 하락
- K=5: 품질과 비용의 균형이 가장 좋음
- K=10: 비용은 2배인데 품질 향상은 미미함

프로젝트 상황에 따라 다를 수 있습니다. A/B 테스트로 최적값을 찾아보세요.

### Q2: 캐시는 언제 만료되나요?

**A:** 30분(TTL)으로 설정되어 있습니다.
- 너무 길면: 최신 정보 반영 안 됨
- 너무 짧으면: 캐시 적중률 낮아짐
- 30분: 인기 검색어의 평균 주기 고려

### Q3: RRF 점수가 0.02 미만인 도서는 왜 버리나요?

**A:** 노이즈 제거를 위해서입니다.
- 0.02 미만: 검색어와 거의 관련 없음
- 0.02~0.05: 약간 관련 있음
- 0.05 이상: 강한 관련성

임계값은 데이터에 따라 조정해야 합니다.

---

## 정리

**핵심 메시지:**

1. **AI 서비스의 3대 문제**: 느림, 비싼, 품질 vs 비용 딜레마
2. **해결책 1 - 시맨틱 캐싱**: 반복 질문 → 99% 비용 절감
3. **해결책 2 - Top-K 최적화**: 토큰 폭발 → 95% 비용 절감
4. **시너지**: 두 기술 결합 → 97.5% 비용 절감

**실제 프로젝트 적용:**
- ✅ 시맨틱 캐싱: 03.semantic-caching.md에서 상세 구현
- ✅ Top-K 최적화: 02.topk-optimization.md에서 상세 구현

---

## 📖 참고자료

### 내부 문서 (구현 상세)
- [02. Top-K와 컨텍스트 최적화](02.topk-optimization.md) - 상세 구현
- [03. 시맨틱 캐싱](03.semantic-caching.md) - 상세 구현

### 공식 문서
- [Google Gemini Pricing](https://ai.google.dev/pricing)
- [Redis Caching Best Practices](https://redis.io/docs/manual/patterns/caching/)
