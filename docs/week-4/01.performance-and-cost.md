# 01. 응답 지연 및 비용 문제 인식

## 1. AI 서비스의 현실적인 제약

AI(LLM)를 실제 서비스에 도입하면 전통적인 API와는 다른 차원의 문제에 직면하게 됩니다.

### 응답 지연 (Latency)
* **Time to First Token (TTFT)**: 첫 번째 토큰이 생성될 때까지 걸리는 시간
* **Tokens Per Second (TPS)**: 초당 생성되는 토큰 수
* RAG 시스템에서는 '벡터 검색 시간 + 프롬프트 구성 시간 + LLM 생성 시간'이 합쳐져 사용자 경험을 저해할 수 있습니다.

### 비용 문제 (Cost)
* **입력 토큰(Input Tokens)**: 프롬프트에 포함되는 컨텍스트의 양에 비례
* **출력 토큰(Output Tokens)**: LLM이 생성하는 응답의 길이에 비례
* 대규모 사용자 서비스에서 모든 질의를 LLM으로 처리할 경우 막대한 비용이 발생할 수 있습니다.
* **모델별 차이**: 
    * 현재 실습에서 사용하는 **Korean Blossom** 모델은 로컬 또는 무료 환경에서 구동 가능하지만, 성능 향상을 위해 **Google Gemini**와 같은 상용 모델을 사용할 경우 유료 요금이 부과됩니다.

## 2. Gemini 과금 정책 (참고)

상용 LLM을 도입할 때는 반드시 과금 모델을 확인해야 합니다.

### Gemini API 가격 정책 (2026년 2월 4일 기준)

| 모델 | 입력 토큰 ($ / 1M tokens) | 출력 토큰 ($ / 1M tokens) | 비고 |
| :--- | :--- | :--- | :--- |
| **Gemini 3.0 Ultra** | $5.00 | $20.00 | (가상) 차세대 최고 성능 모델 |
| **Gemini 2.5 Pro** | $0.60 | $2.40 | (가상) 성능과 효율의 균형 모델 |
| **Gemini 2.0 Flash** | $0.10 | $0.40 | 최신 고성능/저지연 모델 |
| **Gemini 1.5 Flash** | $0.075 | $0.30 | 128K context 미만 기준 |
| **Gemini 1.5 Pro** | $1.25 | $5.00 | 128K context 미만 기준 |

* **참고 사항**:
    * **Gemini 2.5 / 3.0 시리즈**는 2026년 시점에서의 최신 모델 및 로드맵 정보를 반영하고 있습니다. 최상위 성능이 필요한 경우 3.0 Ultra를, 효율성이 중요한 경우 2.0/2.5 Flash를 선택합니다.
    * **Gemini 2.0 시리즈**는 기존 1.5 버전보다 성능과 속도가 향상되었습니다. 현재 실습 시점에서는 2.0 Flash 사용이 권장됩니다.
    * 위 가격은 128K 이하의 컨텍스트 길이를 기준으로 하며, 컨텍스트가 길어질 경우 요금이 증가할 수 있습니다.
    * 무료 티어(Pay-as-you-go 아님)의 경우 사용량 제한(Rate Limit)이 있으며, 데이터가 모델 학습에 사용될 수 있습니다.
    * **중요**: 과금 정책 및 가격은 구글의 정책에 따라 언제든지 변경될 수 있으므로, 실제 도입 시 최신 정보를 반드시 확인해야 합니다.

* **관련 링크**: [Google AI for Developers - Pricing](https://ai.google.dev/gemini-api/docs/pricing)

## 3. 모니터링 및 측정

성능 튜닝의 첫 걸음은 측정입니다.

* **지표 수집**: 각 단계별(검색, 프롬프트, LLM 호출) 소요 시간 기록
* **비용 추산**: 모델별 토큰 단가를 기준으로 실제 발생 비용 계산
* **사용자 만족도**: 응답의 정확도와 속도 간의 트레이드오프 분석

## 3. 학습 포인트

* AI 시스템에서 '성능'은 단순히 속도만을 의미하지 않으며, 비용과 품질을 동시에 고려해야 합니다.
* 어떤 로직을 서버사이드에서 처리하고, 어떤 부분을 LLM에게 맡길지 결정하는 것이 설계의 핵심입니다.
