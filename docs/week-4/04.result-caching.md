# 04. 결과 캐싱 (Semantic Caching)

## 1. AI 응답 캐싱의 필요성

전통적인 캐싱(Exact Match)은 동일한 질문에 대해서만 작동하지만, AI 서비스에서는 의미가 유사한 질문에 대해서도 캐시를 활용할 수 있어야 합니다.

### 이점
* **응답 속도 향상**: LLM 호출 없이 수 밀리초 내에 응답 가능
* **비용 절감**: 고비용의 LLM API 호출 횟수 감소
* **동일 사용자 경험 제공**: 반복되는 질문에 대해 일관된 답변 보장

## 2. 시맨틱 캐싱 (Semantic Caching) 원리

1. 사용자의 질문을 임베딩 벡터로 변환합니다.
2. 캐시 데이터베이스(벡터 DB)에서 유사도가 높은 기존 질문이 있는지 검색합니다.
3. 설정한 임계값(Threshold, 예: 0.95 이상)을 넘는 질문이 있다면 저장된 응답을 반환합니다.
4. 없다면 LLM을 호출하고 결과를 캐시에 저장합니다.

## 3. 주의사항 및 한계

* **최신성 문제**: 데이터가 업데이트되었음에도 캐시된 과거 응답이 나갈 수 있습니다. (TTL 설정 필요)
* **모호성**: 질문의 의도가 미묘하게 다를 경우(예: '추천해줘' vs '비추천해줘') 잘못된 캐시가 작동할 위험이 있습니다.

## 4. 학습 포인트

* 벡터 검색의 응용 사례로서 시맨틱 캐싱의 개념을 이해합니다.
* 성능 최적화를 위해 애플리케이션 계층에서 도입할 수 있는 아키텍처 패턴을 익힙니다.
