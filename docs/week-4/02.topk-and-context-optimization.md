# 02. Top-K 및 Context 최적화 (실습 가이드)

## 1. 실습 개요

이전 단계에서 우리는 하이브리드 검색을 통해 도서를 찾아내고, 그 결과를 LLM에게 전달하여 답변을 생성하는 RAG 시스템을 구축했습니다.

### ❓ RAG 검색에서 UI 페이징은 무의미할까?
일반적으로 웹 서비스에서 사용하는 '페이징(PageSize)'은 사용자가 한 번에 보기 편한 양(예: 10개씩 보기)을 조절하는 도구입니다. 하지만 RAG 시스템에서는 다음과 같은 이유로 **"검색 결과의 페이지 크기"와 "AI에게 전달할 컨텍스트의 크기"**를 분리해서 생각해야 합니다.

1.  **AI는 1페이지 정보만으로 부족할 수 있습니다**: 사용자가 10개씩 보기로 설정했더라도, 질문에 대한 정확한 답은 11위~20위 문서에 있을 수 있습니다. 따라서 AI는 더 넓은 범위(Retrieval K)를 훑어봐야 합니다.
2.  **AI는 10개를 다 읽을 필요가 없을 수도 있습니다**: 검색 결과가 10개라고 해서 AI에게 10개를 다 주면, 관련 없는 정보(Noise) 때문에 오히려 답변 품질이 떨어지고 비용만 올라갑니다.

이 과정에서 사용자 화면을 위한 'UI 페이징' 처리는 AI 응답의 품질을 결정하는 `Retrieval K`와는 완전히 분리되어야 합니다. 또한, RAG 검색의 목적은 가장 적합한 답변을 AI가 찾아주는 것이므로, 사용자가 여러 페이지를 넘겨가며 도서를 직접 찾는 전통적인 페이징 UI는 RAG 모드에서는 제거하거나 비활성화하는 것이 사용자 경험 측면에서 더 자연스럽습니다.

---

## 2. [미션 1] Retrieval K와 Rerank K의 분리

현재 우리 시스템은 `pageSize`에 따라 검색된 결과를 그대로 LLM에게 보냅니다. 하지만 검색 엔진에서 충분한 후보군을 뽑는 것(Retrieval)과, LLM이 집중해서 읽어야 할 최상위 문서를 고르는 것(Rerank)은 분리되어야 합니다.

### 구현 목표
- 검색 엔진에서는 충분한 양(예: 100개)의 데이터를 가져오되(Retrieval K),
- LLM에게는 그중 가장 관련성이 높은 최상위 N개만 전달하도록(Rerank K) 코드를 수정합니다.

### 가이드 (BookSearchService.java)
1. `generateAiResponse` 메서드 호출 시, 전체 리스트가 아닌 상위 `K`개만 슬라이싱하여 전달하도록 수정하세요.
2. `K`값은 질문의 성격에 따라 다를 수 있지만, 기본적으로 5~10 사이의 값을 권장합니다.
    - **이유**: 5개 미만은 충분한 정보를 제공하지 못할 수 있고, 10개 이상은 컨텍스트가 너무 길어져 비용이 증가하고 핵심 정보가 희석(Noise 증가)될 위험이 있기 때문입니다. 5~10개는 답변의 품질과 비용 효율성을 동시에 잡을 수 있는 최적의 범위입니다.

```java
// 변경 전
aiResponse = generateAiResponse(request.keyword(), results.getContent());

// 변경 후 (예시: 상위 5개만 선별)
List<BookSearchResponse> topKBooks = results.getContent().stream()
    .limit(5) 
    .toList();
aiResponse = generateAiResponse(request.keyword(), topKBooks);
```

---

## 3. [미션 2] RRF Score Threshold 필터링

단순히 상위 N개를 가져오는 것보다 중요한 것은 **"관련 없는 정보는 과감히 버리는 것"** 입니다. RRF 점수가 너무 낮은 문서는 오히려 LLM에게 혼란(Noise)을 줄 수 있습니다.

### 구현 목표
- RRF 점수가 특정 임계값(Threshold) 이하인 도서는 검색 결과에는 포함되더라도, AI 컨텍스트에서는 제외하는 로직을 추가합니다.

### 임계값(Threshold)의 의미와 필요성
RRF(Reciprocal Rank Fusion) 점수는 여러 검색 알고리즘(키워드 검색, 벡터 검색 등)의 순위를 통합하여 계산된 **상대적인 관련성 지표**입니다. 

임계값 필터링을 도입하는 이유는 다음과 같습니다:
1. **노이즈 제거 (Noise Reduction)**: 검색 엔진은 질문과 전혀 상관없는 문서라도 '그나마' 가까운 것들을 순위대로 가져옵니다. 점수가 현저히 낮은 문서는 질문에 대한 답을 포함하고 있지 않을 가능성이 높으며, 이를 LLM에게 전달하면 엉뚱한 답변(Hallucination)을 유도할 수 있습니다.
2. **비용 최적화**: 관련 없는 문서를 LLM에게 보내는 것은 불필요한 입력 토큰 비용을 발생시킵니다. **만약 모든 검색 결과가 임계값보다 낮다면, AI 호출 자체를 생략하여 비용을 100% 아낄 수 있습니다.**
3. **답변 품질 향상**: LLM은 제한된 컨텍스트 안에서 가장 핵심적인 정보에 집중해야 합니다. 쓰레기 데이터(Garbage In)를 걸러냄으로써 결과적으로 더 정확한 답변(Garbage Out 방지)을 얻을 수 있습니다.

실습에서 설정할 `0.02`라는 값은 약 20~30위권 밖의 문서들이 가지는 점수대로, 상위권 문서들과의 격차가 큰 '관련성 낮은' 문서들을 걸러내는 기준점이 됩니다. **이 점수를 넘는 문서가 하나도 없다면, AI는 "모르는 내용"에 대해 억지로 답하지 않고 응답 생성을 건너뛰게 됩니다.**

### 가이드 (BookSearchService.java)
1. `generateAiResponse`에 데이터를 넘기기 전, `rrfScore`를 체크하는 로직을 추가하세요.
2. 실습 시 임계값은 `0.02` 정도로 설정해보고 결과의 변화를 관찰하세요.

```java
// 예시 로직
double THRESHOLD = 0.02;
List<BookSearchResponse> filteredBooks = results.getContent().stream()
    .filter(b -> b.getRrfScore() >= THRESHOLD)
    .limit(5)
    .toList();
```

---

## 4. [미션 3] Context 재구성 (Lost in the Middle 방지)

LLM은 컨텍스트의 **처음과 끝**에 있는 정보를 더 잘 처리하고, 중간에 있는 정보는 놓치는 경향이 있습니다. 이를 'Lost in the Middle' 현상이라고 합니다.

### 구현 목표
- 가장 중요한 문서(RRF 점수가 가장 높은 문서)를 컨텍스트의 가장 앞부분에 배치하여 LLM이 이를 최우선적으로 참고하게 합니다.

### 가이드
1. 현재 `generateAiResponse` 내부에서 `StringBuilder context`를 만드는 순서가 RRF 점수 내림차순인지 확인하세요.
2. 필요하다면 `results.getContent()`를 점수 기반으로 다시 한 번 정렬한 뒤 `StringBuilder`에 추가하세요.

## 5. 학습 포인트 (정리)

1.  **Trade-off**: K가 너무 작으면 정답을 놓칠 수 있고, 너무 크면 비용이 늘고 답변이 흐릿해집니다.
2.  **Noise Reduction**: 관련 없는 문서를 제거하는 것이 답변의 정확도(Hallucination 방지)에 직접적인 영향을 줍니다.
3.  **Efficiency**: 토큰을 아끼는 것은 곧 서비스의 지속 가능성(비용)과 직결됩니다.
