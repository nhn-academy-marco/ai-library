# 03. 결과 캐싱 (Result Caching)

AI 응답은 생성될 때마다 비용과 시간이 발생합니다. 이를 효율적으로 관리하기 위해 우리는 **전통적인 캐싱(Spring Cache)** 과 **최신 AI 캐싱(Semantic Caching)** 두 가지 전략을 사용합니다. 특히, 실무에서는 **"자주 묻는 질문에 대한 답변을 미리 만들어 두는 전략"**이 매우  효과적입니다.

---

## 1. 사용자의 통찰: "미리 만들어둔 답변 보여주기"

RAG 검색은 여러 단계를 거치기 때문에 시간이 오래 걸립니다. (하이브리드 검색 -> 컨텍스트 구성 -> LLM 호출 -> 응답 생성)
사용자가 제안한 **"하이브리드 검색 결과 중 빈번한 검색어에 대해 미리 RAG 결과를 캐싱해두는 방식"**은 매우 훌륭한 전략입니다.

### 💡 실무에서는 어떻게 적용할까요?
1.  **인기 검색어 분석**: 실시간 로그 분석을 통해 "자바 추천", "스프링 입문"과 같이 반복되는 인기 키워드를 추출합니다.
2.  **이벤트 기반 비동기 캐시 생성**: 사용자가 검색을 수행할 때 **Spring Event**를 발행합니다. 백그라운드 리스너는 이 이벤트를 받아 비동기적으로 RAG 결과를 생성하여 캐시 DB(또는 Redis/Ehcache)에 저장합니다.
3.  **캐시 우선 반환 및 폴백(Fallback)**: 
    - 동일한 키워드 요청이 들어왔을 때 캐시가 있으면 LLM을 거치지 않고 즉시 반환합니다.
    - **만약 캐시가 없다면?** 사용자를 기다리게 하는 대신, 즉시 **하이브리드 검색 결과**를 먼저 보여주고 백그라운드에서 캐시 생성을 시작합니다. 사용자는 다음번 검색 때 완성된 AI 답변을 보게 됩니다.

---

## 2. Spring Cache: 백엔드의 기본기

Spring Cache는 서비스의 성능을 높이기 위해 자주 사용되는 데이터를 메모리에 임시로 저장하는 기능을 제공합니다.

### 💡 Spring Cache란?
- **추상화(Abstraction)**: 특정 캐시 기술(Ehcache, Redis 등)에 종속되지 않고, 어노테이션만으로 간편하게 캐싱을 적용할 수 있는 기능을 제공합니다.
- **주요 어노테이션**:
    - `@Cacheable`: 메서드 호출 결과를 캐시에 저장합니다. 다음 호출 시 동일한 파라미터가 들어오면 실제 메서드를 실행하지 않고 캐시된 값을 반환합니다.
    - `@CacheEvict`: 캐시에 저장된 데이터를 삭제합니다. (데이터가 수정되거나 삭제될 때 사용)
    - `@CachePut`: 메서드를 실행하고 그 결과를 캐시에 업데이트합니다.

### 🛠 Ehcache: 우리의 캐시 매니저
우리 프로젝트에서는 캐시의 실제 구현체로 **Ehcache**를 사용합니다.
- **Ehcache**: 자바 기반의 오픈소스 캐시 라이브러리로, Spring과 매우 잘 통합되며 설정이 간편합니다.
- **로컬 캐싱**: 외부 서버(Redis 등) 없이 애플리케이션 내부 메모리에 데이터를 저장하므로 지연 시간이 거의 없습니다.

---

## 2. AI 응답 캐싱: 왜 특별할까요?

기존의 검색 시스템에서 캐싱은 **"완벽하게 똑같은 글자"** 가 들어왔을 때만 작동했습니다. 예를 들어 "자바 책 추천"과 "자바 도서 추천"은 사람이 보기엔 같지만, 전통적인 캐싱 시스템은 이를 다른 질문으로 인식했습니다.

**시맨틱 캐싱(Semantic Caching)** 은 글자가 달라도 **"의미"** 가 비슷하면 미리 저장해둔 답변을 꺼내주는 똑똑한 방식입니다.

### 💡 도입 시 이점
*   **번개 같은 속도 (Latency)**: 고비용의 LLM API를 호출하고 응답을 기다릴 필요 없이, DB에서 바로 결과를 가져오므로 수 밀리초(ms) 내에 응답이 가능합니다.
*   **지갑을 지켜주는 비용 절감 (Cost)**: LLM 호출은 돈입니다. 자주 묻는 질문을 캐싱하면 API 비용을 90% 이상 아낄 수 있습니다.
*   **일관된 사용자 경험**: 동일한 질문에 대해 LLM이 매번 조금씩 다른 말을 하는 것을 방지하고, 검증된 고품질의 답변을 일정하게 제공할 수 있습니다.

---

## 3. 시맨틱 캐싱의 마법: 어떻게 동작하나요?

시맨틱 캐싱은 우리가 앞서 배운 **"벡터 검색"** 의 원리를 그대로 활용합니다.

### 💡 코사인 유사도 (Cosine Similarity)
두 질문이 얼마나 비슷한지 계산하기 위해 **코사인 유사도** 공식을 사용합니다. 이 공식은 두 벡터 사이의 각도를 측정하여 방향의 일치 정도를 나타냅니다.

$$ \text{similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \sqrt{\sum_{i=1}^n B_i^2}} $$

- **분자**: 두 벡터의 내적 (방향이 일치할수록 값이 커짐)
- **분모**: 두 벡터의 크기(L2 Norm)의 곱 (결과값을 -1 ~ 1 사이로 정규화)
- **결과**: 1에 가까울수록 두 질문은 의미적으로 거의 동일하다는 뜻입니다.

1.  **질문 변환**: 사용자가 "스프링 강의 추천해줘"라고 물으면, 이 문장을 **벡터(숫자 리스트)** 로 만듭니다.
2.  **유사 질문 검색**: 캐시 DB(벡터 전용 DB)에서 이 벡터와 가장 가까운 위치에 있는 과거 질문이 있는지 찾습니다.
3.  **임계값(Threshold) 판정** : 
    *   유사도가 **0.95(95%) 이상** 이라면? → "아, 이건 예전에 답했던 질문과 사실상 똑같네!"라고 판단하고 저장된 답변을 즉시 반환합니다.
    *   유사도가 낮다면? → "새로운 질문이네."라고 판단하여 LLM을 호출하고, 그 결과와 질문 벡터를 캐시 DB에 새롭게 저장합니다.

---

## 4. 주의사항: 완벽한 기술은 없습니다

시맨틱 캐싱을 쓸 때는 다음을 꼭 고려해야 합니다.

*   **최신성 유지 (TTL)**: 도서 목록이 바뀌었는데 캐시는 한 달 전 정보를 줄 수 있습니다. 일정 시간이 지나면 캐시를 비워주는(Time To Live) 설정이 필요합니다.
*   **의도의 미묘한 차이**: 
    *   "자바 공부법 알려줘" vs "파이썬 공부법 알려줘" → 벡터상으론 가까울 수 있지만 답은 전혀 달라야 합니다.
    *   따라서 **임계값(Threshold)**을 매우 깐깐하게(예: 0.98 이상) 설정하는 것이 안전합니다.
*   **개인화 정보**: 사용자의 이름이나 개인적인 상황이 포함된 질문은 캐싱하면 안 됩니다. 다른 사람에게 유출될 수 있기 때문입니다.

---

## 5. 캐시 유지 정책 (TTL)

캐시된 데이터가 영구적으로 남으면 도서 정보의 변경사항이 반영되지 않거나 메모리 부족 현상이 발생할 수 있습니다. 이를 해결하기 위해 **TTL(Time To Live)** 정책을 도입합니다.

### 💡 TTL 설정 및 구현
1.  **설정 관리**: `application.properties`를 통해 유지 시간을 분 단위로 설정할 수 있습니다.
    ```properties
    # 캐시 유지 시간 (기본값: 30분)
    cache.ttl.minutes=30
    ```
2.  **데이터 구조**: 캐시 저장 시점(`createdAt`)을 결과 데이터와 함께 저장합니다.
3.  **만료 판정**: 캐시 조회 시 현재 시간과 저장 시간을 비교하여 설정된 TTL을 초과했다면 '만료'로 간주하고 새로운 AI 응답을 생성합니다.

---

## 6. 실습 미션: 캐싱 도입하기

### [미션 1] 시맨틱 캐싱 (Semantic Caching) 구현
우리 프로젝트의 `BookSearchService`에 단순 문자열 비교가 아닌 **벡터 유사도 기반의 시맨틱 캐싱**을 적용해 봅시다.

1.  **동작 원리**: 
    - 사용자의 검색어를 벡터로 변환합니다.
    - 캐시 저장소(Map 등)를 순회하며 기존에 저장된 `BookSearchRequest`의 벡터와 현재 요청 벡터 간의 **코사인 유사도**를 계산합니다.
    - 유사도가 임계값(예: **0.98**) 이상이면 동일한 질문으로 간주하고 캐시된 결과를 반환합니다.
2.  **구현 포인트**:
    - `BookSearchRequest` 레코드의 `equals`와 `hashCode`를 재정의하여 벡터 배열의 내용을 비교할 수 있도록 합니다 (`Arrays.equals` 사용).
    - `BookSearchService.findSimilarResultInCache` 메서드를 구현하여 캐시 히트 여부를 판정합니다.
    - 캐시 미스 시에만 하이브리드/RAG 검색을 수행하고 결과를 캐시에 저장합니다.

### [미션 2] 전략적 캐싱: "이벤트 기반 비동기 처리와 폴백"
사용자가 제안한 **"하이브리드 검색을 먼저 수행하고, 캐시가 없을 시 이벤트를 통해 백그라운드에서 RAG 결과를 만드는 전략"**을 구현해 봅시다.

1.  **시나리오**:
    - 사용자가 "자바"라고 검색(RAG 모드)합니다.
    - 시스템은 캐시가 있는지 확인합니다. 
    - **캐시가 없다면**:
        - `BookSearchEvent`를 발행하여 백그라운드에서 AI 분석을 시작합니다.
        - 사용자에게는 기다림 없이 즉시 **하이브리드 검색 결과**를 반환합니다. (AI 추천 영역은 비워둡니다.)
    - **캐시가 있다면**: 저장된 AI 답변과 함께 검색 결과를 0.1초 만에 보여줍니다.

2.  **구현 가이드**:
    - **이벤트 발행**: `ApplicationEventPublisher`를 사용하여 검색 키워드를 담은 이벤트를 발행합니다.
    - **비동기 리스너**: `@EventListener`와 `@Async`를 사용하여 사용자의 응답 흐름과 분리된 별도 스레드에서 RAG 분석을 수행합니다.
    - **중복 작업 방지**: 비동기 리스너(`warmUpRagCache`)에서도 `findSimilarResultInCache`를 호출하여 이미 유사한 캐시가 있다면 추가적인 LLM 호출을 생략하도록 최적화합니다.
    - **폴백 로직**: `BookSearchService`에서 캐시 미스 시 `hybridSearch` 결과를 즉시 반환하고 백그라운드에서 캐시 생성을 유도하는 분기 로직을 작성합니다.

---

## 6. 학습 포인트

1.  **캐싱의 계층화**: 전통적인 Spring Cache(Ehcache)와 AI 기반의 시맨틱 캐싱을 함께 사용하여 시스템의 안정성과 지능을 동시에 확보하는 아키텍처를 이해합니다.
2.  **데이터의 신선도**: TTL 정책을 통해 캐시의 효율성과 정보의 최신성 사이의 균형을 맞추는 방법을 배웁니다.
3.  **벡터 검색의 응용** : 벡터 검색이 단순히 '데이터 찾기'를 넘어 '성능 최적화(캐싱)'에도 쓰일 수 있음을 이해합니다.
4.  **아키텍처 설계** : 무조건 최신 AI 기술을 쓰는 것보다, 캐싱과 같은 전통적인 최적화 기법을 AI와 결합하는 것이 **"진짜 실력 있는 백엔드 개발자"** 의 모습임을 배웁니다.
5.  **임계값의 미학**: 서비스의 정확도와 효율성 사이에서 적절한 유사도 임계값을 결정하는 실험적인 사고방식을 익힙니다.
