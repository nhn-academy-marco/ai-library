# 03. 결과 캐싱 (Result Caching)

AI 응답은 생성될 때마다 비용과 시간이 발생합니다. 이를 효율적으로 관리하기 위해 우리는 **직접적인 캐싱 전략(Semantic Caching)**을 사용합니다. 특히, 실무에서는 **"자주 묻는 질문에 대한 답변을 미리 만들어 두는 전략"**이 매우 효과적입니다.

---

## 1. 사용자의 통찰: "미리 만들어둔 답변 보여주기"

RAG 검색은 여러 단계를 거치기 때문에 시간이 오래 걸립니다. (하이브리드 검색 -> 컨텍스트 구성 -> LLM 호출 -> 응답 생성)
사용자가 제안한 **"하이브리드 검색 결과 중 빈번한 검색어에 대해 미리 RAG 결과를 캐싱해두는 방식"**은 매우 훌륭한 전략입니다.

### 💡 실무에서는 어떻게 적용할까요?
1.  **인기 검색어 분석**: 실시간 로그 분석을 통해 "자바 추천", "스프링 입문"과 같이 반복되는 인기 키워드를 추출합니다.
2.  **이벤트 기반 비동기 캐시 생성**: 사용자가 검색을 수행할 때 **Spring Event**를 발행합니다. 백그라운드 리스너는 이 이벤트를 받아 비동기적으로 RAG 결과를 생성하여 저장합니다.
3.  **캐시 우선 반환 및 폴백(Fallback)**: 
    - 동일한 키워드 요청이 들어왔을 때 캐시가 있으면 LLM을 거치지 않고 즉시 반환합니다.
    - **만약 캐시가 없다면?** 사용자를 기다리게 하는 대신, 즉시 **하이브리드 검색 결과**를 먼저 보여주고 백그라운드에서 캐시 생성을 시작합니다. 사용자는 다음번 검색 때 완성된 AI 답변을 보게 됩니다.

---

## 2. AI 응답 캐싱: 왜 특별할까요?

기존의 검색 시스템에서 캐싱은 **"완벽하게 똑같은 글자"** 가 들어왔을 때만 작동했습니다. 예를 들어 "자바 책 추천"과 "자바 도서 추천"은 사람이 보기엔 같지만, 전통적인 캐싱 시스템은 이를 다른 질문으로 인식했습니다.

**시맨틱 캐싱(Semantic Caching)** 은 글자가 달라도 **"의미"** 가 비슷하면 미리 저장해둔 답변을 꺼내주는 똑똑한 방식입니다.

### 💡 도입 시 이점
*   **번개 같은 속도 (Latency)**: 고비용의 LLM API를 호출하고 응답을 기다릴 필요 없이, DB에서 바로 결과를 가져오므로 수 밀리초(ms) 내에 응답이 가능합니다.
*   **지갑을 지켜주는 비용 절감 (Cost)**: LLM 호출은 돈입니다. 자주 묻는 질문을 캐싱하면 API 비용을 90% 이상 아낄 수 있습니다.
*   **일관된 사용자 경험**: 동일한 질문에 대해 LLM이 매번 조금씩 다른 말을 하는 것을 방지하고, 검증된 고품질의 답변을 일정하게 제공할 수 있습니다.

---

## 3. 시맨틱 캐싱의 마법: 어떻게 동작하나요?

시맨틱 캐싱은 우리가 앞서 배운 **"벡터 검색"** 의 원리를 그대로 활용합니다.

### 💡 코사인 유사도 (Cosine Similarity)
두 질문이 얼마나 비슷한지 계산하기 위해 **코사인 유사도** 공식을 사용합니다. 이 공식은 두 벡터 사이의 각도를 측정하여 방향의 일치 정도를 나타냅니다.

$$ \text{similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \sqrt{\sum_{i=1}^n B_i^2}} $$

- **분자**: 두 벡터의 내적 (방향이 일치할수록 값이 커짐)
- **분모**: 두 벡터의 크기(L2 Norm)의 곱 (결과값을 -1 ~ 1 사이로 정규화)
- **결과**: 1에 가까울수록 두 질문은 의미적으로 거의 동일하다는 뜻입니다.

1.  **질문 변환**: 사용자가 "스프링 강의 추천해줘"라고 물으면, 이 문장을 **벡터(숫자 리스트)** 로 만듭니다.
2.  **유사 질문 검색**: 캐시 저장소에서 이 벡터와 가장 가까운 위치에 있는 과거 질문이 있는지 찾습니다.
3.  **임계값(Threshold) 판정** : 
    *   유사도가 **0.95(95%) 이상** 이라면? → "아, 이건 예전에 답했던 질문과 사실상 똑같네!"라고 판단하고 저장된 답변을 즉시 반환합니다.
    *   유사도가 낮다면? → "새로운 질문이네."라고 판단하여 LLM을 호출하고, 그 결과와 질문 벡터를 캐시 저장소에 새롭게 저장합니다.

---

## 4. 주의사항: 완벽한 기술은 없습니다

시맨틱 캐싱을 쓸 때는 다음을 꼭 고려해야 합니다.

*   **최신성 유지 (TTL)**: 도서 목록이 바뀌었는데 캐시는 한 달 전 정보를 줄 수 있습니다. 일정 시간이 지나면 캐시를 비워주는(Time To Live) 설정이 필요합니다.
*   **의도의 미묘한 차이**: 
    *   "자바 공부법 알려줘" vs "파이썬 공부법 알려줘" → 벡터상으론 가까울 수 있지만 답은 전혀 달라야 합니다.
    *   따라서 **임계값(Threshold)**을 매우 깐깐하게(예: 0.98 이상) 설정하는 것이 안전합니다.
*   **개인화 정보**: 사용자의 이름이나 개인적인 상황이 포함된 질문은 캐싱하면 안 됩니다. 다른 사람에게 유출될 수 있기 때문입니다.

---

## 5. 캐시 유지 정책 (TTL)

캐시된 데이터가 영구적으로 남으면 도서 정보의 변경사항이 반영되지 않거나 메모리 부족 현상이 발생할 수 있습니다. 이를 해결하기 위해 **TTL(Time To Live)** 정책을 도입합니다.

### 💡 TTL 설정 및 구현
1.  **설정 관리**: `application.properties`를 통해 유지 시간을 분 단위로 설정할 수 있습니다.
    ```properties
    # 캐시 유지 시간 (기본값: 30분)
    cache.ttl.minutes=30
    ```
2.  **데이터 구조**: 캐시 저장 시점(`createdAt`)을 결과 데이터와 함께 저장합니다.
3.  **만료 판정**: 캐시 조회 시 현재 시간과 저장 시간을 비교하여 설정된 TTL을 초과했다면 '만료'로 간주하고 새로운 AI 응답을 생성합니다.

---

## 6. 구현 가이드 (Step-by-Step)

### 3.1. 캐시 엔티티(Entity) 및 리포지토리 정의
캐시된 검색 결과와 질문 벡터를 저장하기 위한 구조를 설계합니다.

**BookSearchCache.java**
```java
@Getter
@Builder
public class BookSearchCache {
    private String keyword;
    private float[] vector;
    private List<BookSearchResponse> books;
    private String aiResponse;
    private long createdAt;
}
```

### 3.2. 시맨틱 캐시 서비스(SemanticCacheService) 구현
질문 간의 유사도를 계산하고 캐시 히트 여부를 판정하는 로직을 작성합니다.

**SemanticCacheService.java**
```java
@Service
@RequiredArgsConstructor
public class SemanticCacheService {
    private final BookSearchCacheRepository cacheRepository;
    private static final double SIMILARITY_THRESHOLD = 0.98;

    public Optional<BookSearchResult> findSimilarResult(BookSearchRequest request) {
        Iterable<BookSearchCache> allCached = cacheRepository.findAll();

        for (BookSearchCache cached : allCached) {
            // 코사인 유사도 계산
            double similarity = calculateCosineSimilarity(request.vector(), cached.getVector());
            
            if (similarity >= SIMILARITY_THRESHOLD) {
                // TTL 만료 체크 후 결과 반환
                return Optional.of(BookSearchResult.fromCache(cached));
            }
        }
        return Optional.empty();
    }
}
```

### 3.3. 이벤트 기반 비동기 워밍업(Warm-up)
캐시가 없을 때 백그라운드에서 AI 응답을 미리 생성하도록 이벤트를 활용합니다.

**BookSearchEventListener.java**
```java
@Component
@RequiredArgsConstructor
public class BookSearchEventListener {
    private final BookSearchCacheService cacheService;

    @Async // 비동기 처리
    @EventListener
    public void handleSearchEvent(BookSearchEvent event) {
        // 캐시가 없는 경우에만 백그라운드에서 RAG 분석 수행
        cacheService.warmUpRagCache(event.getKeyword());
    }
}
```

### 3.4. 서비스 계층 통합 (Fallback 로직)
캐시가 있으면 즉시 반환하고, 없으면 하이브리드 검색 결과를 먼저 보여주며 이벤트를 발행합니다.

**BookSearchService.java**
```java
public BookSearchResult searchBooks(Pageable pageable, BookSearchRequest request) {
    // 1. 시맨틱 캐시 확인
    Optional<BookSearchResult> cached = semanticCacheService.findSimilarResult(request);
    if (cached.isPresent()) return cached.get();

    // 2. 캐시 미스 시 하이브리드 검색 수행
    BookSearchResult hybridResult = hybridSearch(pageable, request);

    // 3. 비동기 캐시 생성을 위한 이벤트 발행 (Fallback)
    eventPublisher.publishEvent(new BookSearchEvent(request.keyword()));

    return hybridResult;
}
```

---

## 6. 학습 포인트

1.  **직접 구현하는 캐싱 전략**: 추상화된 프레임워크 기능에만 의존하지 않고, 서비스 요구사항에 맞는 시맨틱 캐싱을 직접 설계하고 구현하는 능력을 기릅니다.
2.  **데이터의 신선도**: TTL 정책을 통해 캐시의 효율성과 정보의 최신성 사이의 균형을 맞추는 방법을 배웁니다.
3.  **벡터 검색의 응용** : 벡터 검색이 단순히 '데이터 찾기'를 넘어 '성능 최적화(캐싱)'에도 쓰일 수 있음을 이해합니다.
4.  **아키텍처 설계** : 무조건 최신 AI 기술을 쓰는 것보다, 캐싱과 같은 전통적인 최적화 기법을 AI와 결합하는 것이 **"진짜 실력 있는 백엔드 개발자"** 의 모습임을 배웁니다.
5.  **임계값의 미학**: 서비스의 정확도와 효율성 사이에서 적절한 유사도 임계값을 결정하는 실험적인 사고방식을 익힙니다.
