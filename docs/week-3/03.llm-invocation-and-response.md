# 03. 생성과 응답: LLM 호출 및 도서 추천 결과 생성

Week 3의 마지막 단계로, 준비된 컨텍스트와 질문을 AI 모델(LLM)에 전달하고 최종적으로 사용자에게 도서 추천이나 요약 정보를 제공하는 과정을 학습합니다.

## 1. 개요

이제 모든 재료가 준비되었습니다. 검색을 통해 얻은 '지식(Context)'과 사용자의 '의도(Question)'를 섞어서 AI에게 전달하면, AI는 이를 바탕으로 자연스러운 문장을 만들어냅니다. 이 과정에서 AI 모델을 어떻게 호출하고, 응답을 어떻게 처리하는지 백엔드 구현 관점에서 살펴봅니다.

## 2. 주요 개념

### 2.1. Chat Completion API
AI 모델과 대화하듯이 요청을 보내고 응답을 받는 API입니다. 
- **Role (역할)**: 시스템(System), 사용자(User), 어시스턴트(Assistant) 등으로 메시지의 성격을 정의합니다.
- **Tokens (토큰)**: AI가 텍스트를 처리하는 기본 단위입니다. 문장의 길이에 따라 비용이 발생합니다.

### 2.2. Temperature (온도)
AI 응답의 '창의성'을 조절하는 파라미터입니다. 
- **낮은 값 (0.0 ~ 0.3)**: 일관되고 사실적인 답변에 유리 (도서 추천에 적합)
- **높은 값 (0.7 ~ 1.0)**: 다양하고 창의적인 답변에 유리

## 3. 구현 가이드 (Step-by-Step)

### 3.1. 의존성 설정 (pom.xml)
RAG 시스템 구축을 위해 Google GenAI와 OpenAI(임베딩용) 의존성을 추가합니다.

```xml
<!-- Google Gemini 지원 -->
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-starter-model-google-genai</artifactId>
</dependency>

<!-- OpenAI 기반 임베딩 지원 -->
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-openai</artifactId>
</dependency>
```

### 3.2. Spring AI Gemini 및 Ollama 설정
`application.properties`에 사용할 AI 모델에 대한 설정을 추가합니다. 본 프로젝트에서는 Gemini와 Ollama를 선택하여 사용할 수 있도록 구성되어 있습니다.

```properties
# ===============================
# Google gemini
# ===============================
spring.ai.google.genai.api-key=YOUR_API_KEY
spring.ai.google.genai.chat.options.model=gemini-2.0-flash
spring.ai.google.genai.chat.options.temperature=0.3
spring.ai.google.genai.project-id=unused

# ===============================
# Ollama
# ===============================
spring.ai.ollama.base-url=http://ollama.java21.net
spring.ai.ollama.chat.options.model=llama3-korean-blossom
spring.ai.ollama.chat.options.temperature=0.3

# 선택할 AI 모델 (gemini 또는 ollama)
spring.ai.selected-model=ollama
```

### 3.3. AI 클라이언트 서비스 구현
Spring AI의 `ChatModel`을 사용하여 AI 모델과 통신하는 서비스를 작성합니다. `@Primary` 설정을 통해 `application.properties`에서 선택된 모델이 자동으로 주입되도록 할 수 있습니다.

```java
@Service
@RequiredArgsConstructor
public class BookAiService {

    private final ChatModel chatModel;

    public String askAboutBooks(String promptText) {
        return chatModel.call(promptText);
    }
}
```

### 3.4. 서비스 레이어 구현
`BookSearchService`에서 하이브리드 검색 결과와 프롬프트를 조합하여 AI에게 질의하는 로직을 구현합니다.

1.  **사용자 질문 수신**: RAG 검색 타입 확인.
2.  **하이브리드 검색 실행**: 키워드와 벡터 검색을 결합하여 최적화된 도서 리스트를 획득합니다.
3.  **컨텍스트 구성**: 검색된 도서의 ID, 제목, 저자, 출판일, 내용을 텍스트로 결합합니다.
4.  **프롬프트 생성**: `String.replace()` 등을 사용하여 템플릿에 질문과 컨텍스트를 주입합니다.
5.  **AI 모델 호출**: `BookAiService`를 통해 답변을 요청합니다.
6.  **JSON 데이터 파싱**: AI의 응답(JSON)을 `ObjectMapper`를 사용하여 DTO 리스트로 변환합니다.
7.  **최종 결과 반환**: 검색된 도서와 AI 추천 결과를 함께 반환합니다.

### 3.3. 결과 표시 (UI)
AI의 답변을 화면에 출력합니다. 이때 답변의 근거가 된 도서 리스트를 함께 보여주면 사용자의 신뢰도를 높일 수 있습니다.

## 4. 학습 포인트

*   **생성 모델의 특징**: 동일한 질문에도 매번 미세하게 다른 답변이 나올 수 있음을 이해합니다.
*   **응답 제어**: 프롬프트의 지시사항이 AI의 응답 형식(예: "반드시 ~입니다 체를 사용하세요")에 어떻게 반영되는지 확인합니다.
*   **비용 인식**: 하나의 질문을 처리하기 위해 발생하는 토큰 사용량을 확인하고 서비스 운영 관점에서의 비용을 고민해 봅니다.

## 5. 심화 학습 (Study Topics)

### 5.1. 스트리밍 응답 (Streaming)
AI가 답변을 한꺼번에 주지 않고, 한 글자씩 실시간으로 화면에 뿌려주는 방식입니다. 
- **장점**: 사용자가 답변을 기다리는 지루함을 줄여줍니다 (UX 개선).

### 5.2. 환각(Hallucination) 방지 테스트
컨텍스트에 없는 엉뚱한 책 이름을 물어봤을 때, AI가 "모르는 책입니다"라고 정직하게 답하는지, 아니면 거짓말을 하는지 테스트해 보세요. 프롬프트에 "정보가 없으면 모른다고 답하세요"라는 문구가 있고 없고의 차이를 관찰해 봅니다.

## 6. 참고 링크

*   [Spring AI Chat Client](https://docs.spring.io/spring-ai/reference/api/chatclient.html)
*   [Google Gemini API Documentation](https://ai.google.dev/docs)
*   [Spring AI Google AI Gemini Support](https://docs.spring.io/spring-ai/reference/api/chat/google-ai.html)
